{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Required modules"
      ],
      "metadata": {
        "id": "Gr0rvHOgzzAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YMWlTEbprB-f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "X6UrMyI0z9A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "-aynxJATrPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Kaggle_Text_Classification/Coronavirus_tweets.zip"
      ],
      "metadata": {
        "id": "14rTS_HErbzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv', data_files={'train': 'Corona_NLP_train.csv', 'test': 'Corona_NLP_test.csv'}, encoding = \"ISO-8859-1\")"
      ],
      "metadata": {
        "id": "acunw1MytGf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "tZ-G49ne0HH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT model for Covid-19 tweet classification"
      ],
      "metadata": {
        "id": "B_emOjNw0CCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Call pretrained model \n",
        "# 2. Call the tokenizer \n",
        "from transformers import AutoTokenizer\n",
        "model_name = \"bert-base-cased\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "8VjG1WP5twxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See how the BERT tokenizer works (tokenizer used is WordPiece)\n",
        "text = \"Covid-19 has finally ended!\"\n",
        "bert_tokens = bert_tokenizer(text).tokens()\n",
        "print(bert_tokens)"
      ],
      "metadata": {
        "id": "gtnzXY28xo9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels given in the form of strings to integers\n",
        "def label2int(label):\n",
        "    label = label['Sentiment']\n",
        "    idx = 0\n",
        "    if label == 'Positive':\n",
        "        idx = 0\n",
        "    elif label == 'Negative':\n",
        "        idx = 1\n",
        "    elif label == 'Neutral':\n",
        "        idx = 2\n",
        "    elif label == 'Extremely Positive':\n",
        "        idx = 3\n",
        "    elif label == 'Extremely Negative':\n",
        "        idx = 4\n",
        "    return {'labels': idx}\n",
        "\n",
        "# Tokenize the entire data\n",
        "def tokenize_data(example):\n",
        "    return bert_tokenizer(example['OriginalTweet'], padding='max_length')\n",
        "\n",
        "\n",
        "dataset = dataset.map(tokenize_data, batched=True)\n",
        "\n",
        "remove_columns = ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']\n",
        "dataset = dataset.map(label2int, remove_columns=remove_columns)"
      ],
      "metadata": {
        "id": "UzBTNt1gx6jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "7vMJ9VXxKjfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model and training args using Hugging face transformers library\n",
        "from transformers import TrainingArguments, AutoModelForSequenceClassification\n",
        "args_train = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)"
      ],
      "metadata": {
        "id": "yTPylgDwyl4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40K train and 1k test split \n",
        "train_dataset = dataset['train'].shuffle(seed=0).select(range(40000))\n",
        "eval_dataset = dataset['train'].shuffle(seed=0).select(range(40000, 41000))"
      ],
      "metadata": {
        "id": "m4Ik29Uqy8mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the BERT model on text classification task\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(model=model, args=args_train, train_dataset=train_dataset, eval_dataset=eval_dataset)"
      ],
      "metadata": {
        "id": "U8HVfEQTzD4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "RPuyVdflzOJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "bcXBtsKtzQx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}